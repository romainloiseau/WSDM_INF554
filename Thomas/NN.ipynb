{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('../output_preprocessed/test_last_transaction.csv')\n",
    "test_features = test[test.columns[2:]]\n",
    "\n",
    "predictions = pd.DataFrame()\n",
    "predictions['msno']=test['msno']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../output_preprocessed/train_last_transaction.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          1\n",
       "1          1\n",
       "2          1\n",
       "3          1\n",
       "4          1\n",
       "5          1\n",
       "6          1\n",
       "7          1\n",
       "8          1\n",
       "9          1\n",
       "10         1\n",
       "11         1\n",
       "12         1\n",
       "13         1\n",
       "14         1\n",
       "15         1\n",
       "16         1\n",
       "17         1\n",
       "18         1\n",
       "19         1\n",
       "20         1\n",
       "21         1\n",
       "22         1\n",
       "23         1\n",
       "24         1\n",
       "25         1\n",
       "26         1\n",
       "27         1\n",
       "28         1\n",
       "29         1\n",
       "          ..\n",
       "1963861    0\n",
       "1963862    0\n",
       "1963863    0\n",
       "1963864    0\n",
       "1963865    0\n",
       "1963866    0\n",
       "1963867    0\n",
       "1963868    0\n",
       "1963869    0\n",
       "1963870    0\n",
       "1963871    0\n",
       "1963872    0\n",
       "1963873    0\n",
       "1963874    0\n",
       "1963875    0\n",
       "1963876    0\n",
       "1963877    0\n",
       "1963878    0\n",
       "1963879    0\n",
       "1963880    0\n",
       "1963881    0\n",
       "1963882    0\n",
       "1963883    0\n",
       "1963884    0\n",
       "1963885    0\n",
       "1963886    0\n",
       "1963887    0\n",
       "1963888    0\n",
       "1963889    0\n",
       "1963890    0\n",
       "Name: is_churn, Length: 1963891, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.is_churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# desired_apriori=0.10\n",
    "\n",
    "# # Get the indices per target value\n",
    "# idx_0 = train[train.is_churn == 0].index\n",
    "# idx_1 = train[train.is_churn == 1].index\n",
    "\n",
    "\n",
    "\n",
    "# # Get original number of records per target value\n",
    "# nb_0 = len(train.loc[idx_0])\n",
    "# nb_1 = len(train.loc[idx_1])\n",
    "\n",
    "# # Calculate the undersampling rate and resulting number of records with target=0\n",
    "# undersampling_rate = ((1-desired_apriori)*nb_1)/(nb_0*desired_apriori)\n",
    "# undersampled_nb_0 = int(undersampling_rate*nb_0)\n",
    "\n",
    "# # Randomly select records with target=0 to get at the desired a priori\n",
    "# undersampled_idx = shuffle(idx_0, random_state=37, n_samples=undersampled_nb_0)\n",
    "\n",
    "# # Construct list with remaining indices\n",
    "# idx_list = list(undersampled_idx) + list(idx_1)\n",
    "\n",
    "# # Return undersample data frame\n",
    "# train = train.loc[idx_list].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150801\n",
      "1813090\n"
     ]
    }
   ],
   "source": [
    "print(len(train[train.is_churn == 1]))\n",
    "print(len(train[train.is_churn == 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: 150801\n",
    "0: 1813090"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train[train.columns[1:]]\n",
    "train_features = train[train.columns[1:]]\n",
    "train_labels = train[\"is_churn\"]\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trans_count</th>\n",
       "      <th>logs_count</th>\n",
       "      <th>city</th>\n",
       "      <th>bd</th>\n",
       "      <th>gender</th>\n",
       "      <th>registered_via</th>\n",
       "      <th>registration_init_time</th>\n",
       "      <th>payment_method_id</th>\n",
       "      <th>payment_plan_days</th>\n",
       "      <th>plan_list_price</th>\n",
       "      <th>...</th>\n",
       "      <th>membership_expire_date</th>\n",
       "      <th>is_cancel</th>\n",
       "      <th>date</th>\n",
       "      <th>num_25</th>\n",
       "      <th>num_50</th>\n",
       "      <th>num_75</th>\n",
       "      <th>num_985</th>\n",
       "      <th>num_100</th>\n",
       "      <th>num_unq</th>\n",
       "      <th>total_secs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20050406.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20170121.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>19799.702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20050407.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20170319.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>281.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20051016.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20161225.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>15845.692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20051102.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20170426.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20170331.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>6171.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20051228.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>477.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20170528.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20170331.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3132.042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   trans_count  logs_count  city    bd  gender  registered_via  \\\n",
       "0            2         0.0  18.0  36.0     2.0             9.0   \n",
       "1           23         1.0  10.0  38.0     1.0             9.0   \n",
       "2           10         0.0  11.0  27.0     2.0             9.0   \n",
       "3            3         5.0  13.0  23.0     2.0             9.0   \n",
       "4            9        17.0   3.0  27.0     1.0             9.0   \n",
       "\n",
       "   registration_init_time  payment_method_id  payment_plan_days  \\\n",
       "0              20050406.0                0.0                0.0   \n",
       "1              20050407.0                0.0                0.0   \n",
       "2              20051016.0                0.0                0.0   \n",
       "3              20051102.0               40.0               30.0   \n",
       "4              20051228.0               38.0               90.0   \n",
       "\n",
       "   plan_list_price     ...      membership_expire_date  is_cancel        date  \\\n",
       "0              0.0     ...                         0.0        0.0  20170121.0   \n",
       "1              0.0     ...                         0.0        0.0  20170319.0   \n",
       "2              0.0     ...                         0.0        0.0  20161225.0   \n",
       "3            149.0     ...                  20170426.0        0.0  20170331.0   \n",
       "4            477.0     ...                  20170528.0        0.0  20170331.0   \n",
       "\n",
       "   num_25  num_50  num_75  num_985  num_100  num_unq  total_secs  \n",
       "0     4.0     0.0     2.0      5.0     76.0     74.0   19799.702  \n",
       "1     0.0     0.0     0.0      0.0      1.0      1.0     281.600  \n",
       "2    61.0    21.0     9.0     11.0     44.0    130.0   15845.692  \n",
       "3    28.0     4.0     5.0      4.0     19.0     51.0    6171.145  \n",
       "4     0.0     0.0     0.0      2.0     11.0     12.0    3132.042  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.076786848149922787"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_labels == 1).sum()/len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['trans_count', 'logs_count', 'city', 'bd', 'gender', 'registered_via',\n",
       "       'registration_init_time', 'payment_method_id', 'payment_plan_days',\n",
       "       'plan_list_price', 'actual_amount_paid', 'is_auto_renew',\n",
       "       'transaction_date', 'membership_expire_date', 'is_cancel', 'date',\n",
       "       'num_25', 'num_50', 'num_75', 'num_985', 'num_100', 'num_unq',\n",
       "       'total_secs'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replacemean(x, t, mean):\n",
    "    if(x < t):\n",
    "        return mean\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "mean_registration_init_time = train_features[train_features[\"registration_init_time\"] > 1][\"registration_init_time\"].mean()\n",
    "train_features[\"registration_init_time\"] = train_features[\"registration_init_time\"].apply(lambda x: replacemean(x, 1, mean_registration_init_time))\n",
    "test_features[\"registration_init_time\"] = test_features[\"registration_init_time\"].apply(lambda x: replacemean(x, 1, mean_registration_init_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "mean_transaction_date = train_features[train_features[\"transaction_date\"] > 1][\"transaction_date\"].mean()\n",
    "train_features[\"transaction_date\"] = train_features[\"transaction_date\"].apply(lambda x: replacemean(x, 1, mean_transaction_date))\n",
    "test_features[\"transaction_date\"] = test_features[\"transaction_date\"].apply(lambda x: replacemean(x, 1, mean_transaction_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "mean_date = train_features[train_features[\"date\"] > 1][\"date\"].mean()\n",
    "train_features[\"date\"] = train_features[\"date\"].apply(lambda x: replacemean(x, 1, mean_date))\n",
    "test_features[\"date\"] = test_features[\"date\"].apply(lambda x: replacemean(x, 1, mean_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "cols_to_transform = [\"city\", \"gender\", \"payment_method_id\"]\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(train_features.loc[:][cols_to_transform])\n",
    "\n",
    "ONE_HOT_train = (enc.transform(train_features.loc[:][cols_to_transform]).toarray()).transpose()\n",
    "ONE_HOT_test = (enc.transform(test_features.loc[:][cols_to_transform]).toarray()).transpose()\n",
    "\n",
    "for col in cols_to_transform:\n",
    "    del train_features[col]\n",
    "    del test_features[col]\n",
    "\n",
    "for i in range(0, ONE_HOT_train.shape[0]):\n",
    "    train_features[\"ONE_HOT_\"+str(i)] = ONE_HOT_train[i]\n",
    "    test_features[\"ONE_HOT_\"+str(i)] = ONE_HOT_test[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "norm = Normalizer()\n",
    "norm.fit(train_features)\n",
    "\n",
    "train_features_preprocessed = norm.transform(train_features)\n",
    "test_features_preprocessed = norm.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_logloss(ischurn, pred_proba):\n",
    "    logloss = -((ischurn*np.log(pred_proba)).sum() + ((1 - ischurn)*np.log(1 - pred_proba)).sum())\n",
    "    return (logloss / len(pred_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Ensemble(object):\n",
    "    def __init__(self, n_splits, stacker, base_models):\n",
    "        self.n_splits = n_splits\n",
    "        self.stacker = stacker\n",
    "        self.base_models = base_models\n",
    "\n",
    "    def fit_predict(self, X, y, T):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        T = np.array(T)\n",
    "\n",
    "        folds = list(StratifiedKFold(n_splits=self.n_splits).split(X, y))\n",
    "\n",
    "        S_train = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        S_test = np.zeros((T.shape[0], len(self.base_models)))\n",
    "        for i, clf in enumerate(self.base_models):\n",
    "\n",
    "            S_test_i = np.zeros((T.shape[0], self.n_splits))\n",
    "\n",
    "            for j, (train_idx, test_idx) in enumerate(folds):\n",
    "                X_train = X[train_idx]\n",
    "                y_train = y[train_idx]\n",
    "                X_holdout = X[test_idx]\n",
    "                y_holdout = y[test_idx]\n",
    "\n",
    "                print (\"Fit %s fold %d\" % (str(clf).split('(')[0], j+1))\n",
    "                clf.fit(X_train, y_train)\n",
    "#                cross_score = cross_val_score(clf, X_train, y_train, cv=3, scoring='roc_auc')\n",
    "#                print(\"    cross_score: %.5f\" % (cross_score.mean()))\n",
    "                y_pred = clf.predict_proba(X_holdout)[:,1]                \n",
    "                print(compute_logloss(y_pred,y_holdout))\n",
    "                S_train[test_idx, i] = y_pred\n",
    "                S_test_i[:, j] = clf.predict_proba(T)[:,1]\n",
    "            S_test[:, i] = S_test_i.mean(axis=1)\n",
    "\n",
    "#         results = cross_val_score(self.stacker, S_train, y, cv=3, scoring='roc_auc')\n",
    "#         print(\"Stacker score: %.5f\" % (results.mean()))\n",
    "\n",
    "        self.stacker.fit(S_train, y)\n",
    "        res = self.stacker.predict_proba(S_test)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-907fd8b45fd5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# fix random seed for reproducibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn import ensemble\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "5\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "K = 5\n",
    "# kf = KFold(n_splits=K)\n",
    "kf = StratifiedKFold(n_splits=K)\n",
    "\n",
    "#gbm = XGBClassifier(max_delta_step = 1)\n",
    "#gbm.fit(train_features_preprocessed, train_labels)\n",
    "#predictions[\"pred_gbm\"] = gbm.predict_proba(test_features_preprocessed)[:,1]\n",
    "\n",
    "\n",
    "# lgbm = lgb.LGBMClassifier(n_estimatorsobjective='binary')\n",
    "# lgbm.fit(train_features_preprocessed, train_labels)\n",
    "# predictions[\"pred_lgbm\"] = lgbm.predict_proba(test_features_preprocessed)[:,1]\n",
    "\n",
    "\n",
    "\n",
    "#adb = ensemble.AdaBoostClassifier()\n",
    "#adb.fit(train_features_preprocessed, train_labels)\n",
    "#predictions[\"pred_adb\"] = adb.predict_proba(test_features_preprocessed)[:,1]\n",
    "\n",
    "\n",
    "#lg = LogisticRegression()\n",
    "#lg.fit(train_features_preprocessed, train_labels)\n",
    "#predictions[\"pred_lg\"] = lg.predict_proba(test_features_preprocessed)[:,1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_preprocessed.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/// est=200, et=0.09, depth=-1, col=0.9, sub=0.9/////\n",
      "0.084882461375\n",
      "0.0861616570708\n",
      "0.105847178829\n",
      "0.100262748559\n",
      "0.100147826668\n"
     ]
    }
   ],
   "source": [
    "#max_depth=6, subsample=0.8, colsample_bytree=0.8, reg_alpha =2, reg_lambda = 6,\n",
    "\n",
    "layer1=24\n",
    "layer2=24\n",
    "layer3=1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(24, input_dim=train_features_preprocessed.shape[1], activation='relu'))\n",
    "model.add(Dense(24, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "targets = np.asarray([0.]*test.shape[0])\n",
    "for train_index,test_index in kf.split(train_features_preprocessed,train_labels):\n",
    "    X_train, y_train = train_features_preprocessed[train_index].copy(),train_labels[train_index].copy()\n",
    "    X_test, y_test = train_features_preprocessed[test_index].copy(),train_labels[test_index].copy()\n",
    "    fit_model = model.fit(X_train, y_train, epochs=150, batch_size=10)\n",
    "    y_predict=fit_model.predict_proba(X_test)\n",
    "    targets += fit_model.predict_proba(test_features_preprocessed)[:,1]\n",
    "    del y_test, X_train, X_test, y_train\n",
    "targets /=K\n",
    "test_target = pd.DataFrame()\n",
    "test_target['msno']=test['msno']\n",
    "test_target['is_churn']=targets\n",
    "test_target.to_csv('../output/output_NN_l1/'+str(layer1)+'_l2/'+str(layer2)+'_l3/'+str(layer3)+'.csv', index=False)\n",
    "#predictions.to_csv(\"../output/from_kaggle_kernels_and_calculs.csv\", float_format='%.6f', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit LGBMClassifier fold 1\n",
      "0.0881535376413\n"
     ]
    }
   ],
   "source": [
    "# cl1 = lgb.LGBMClassifier(n_estimators=1000, learning_rate =0.005 , max_depth=-1,subsample = 1, colsample_bytree=0.9,objective='binary')\n",
    "# cl2 = lgb.LGBMClassifier(n_estimators=200, learning_rate =0.09 , max_depth=-1, subsample = 0.9, colsample_bytree=0.9,objective='binary')\n",
    "\n",
    "# lr1 = LogisticRegression()\n",
    "# stack = Ensemble(n_splits=5,\n",
    "#         stacker = lr1,\n",
    "#         base_models = (cl1,cl2))\n",
    "\n",
    "# targets = stack.fit_predict(train_features_preprocessed,train_labels,test_features_preprocessed)[:,1]\n",
    "# test_target = pd.DataFrame()\n",
    "# test_target['msno']=test['msno']\n",
    "# test_target['is_churn']=targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_target.to_csv('../output/output_LGB_cl1_cl2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "/// est=200, et=0.09, depth=-1, col=0.9, sub=0.9/////\n",
    "0.0789238541209\n",
    "0.0796247254159\n",
    "0.120800671696\n",
    "0.125796860132\n",
    "0.126266228716\n",
    "/// est=1000, et=0.005, depth=-1, col=1, sub=0.8/////\n",
    "0.0831707967063\n",
    "0.0840120852989\n",
    "0.130570757225\n",
    "0.136304585383\n",
    "0.136338357307\n",
    "/// est=1000, et=0.005, depth=-1, col=0.9, sub=1/////\n",
    "0.0833001724272\n",
    "0.084050353593\n",
    "0.130303875663\n",
    "0.136283216618\n",
    "0.136293672491\n",
    "///////////////////////////////////////////////////////////////////////////////\n",
    "est=1000 et=0.005 depth=-1\n",
    "0.0832268324013\n",
    "0.0840225550256\n",
    "0.130625156578\n",
    "0.136549348377\n",
    "0.136210738066\n",
    "///////////////////////////////////////////////////////////////////////////////\n",
    "est=1000 et=0.005 depth=7\n",
    "0.0831035400493\n",
    "0.0839660360624\n",
    "0.132396293088\n",
    "0.138398769378\n",
    "0.138883725537\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
